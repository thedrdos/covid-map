#!/usr/bin/env python3# -*- coding: utf-8 -*-"""Created Jul 2020@author: TheDrDOS"""# Clear the Spyder console and variablestry:    from IPython import get_ipython    #get_ipython().magic('clear')    get_ipython().magic('reset -f')except:    passimport osimport pickleimport progress_bar as pbarimport hashlibimport jsonimport multiprocessing as mpfrom time import time as nowfrom time import perf_counter as pnowT0 = now()# %% '''------------------------------------------------------------------------------Load Data------------------------------------------------------------------------------'''print("Load Data:")t0=pnow()data_path = './tmp_data/'data = pickle.load(open(data_path+'tmp_data_and_maps.p','rb'))print("    Loading Completed in :{} sec".format(pnow()-t0))# From: (assign the keys as variables in the workspace)# data = {#     'covid_data': covid_data,#     'GraphData':GraphData,     #     'MapData':MapData,#     'Type_to_LocationNames':Type_to_LocationNames,#     'LocationName_to_Type':LocationName_to_Type,#         }for d in data:    globals()[d] = data[d]del data# %% '''------------------------------------------------------------------------------Support Functions------------------------------------------------------------------------------'''def short_hash(st,digits=8,num_only=False):    ''' Generate a short hash from a string '''    if num_only:        if digits>49:            digits=49;        return int(hashlib.sha1(st.encode()).hexdigest(),16)%(10**digits)    else:        if digits>40:            digits=40;        return hashlib.sha1(st.encode()).hexdigest().upper()[0:digits-1]def mp_dump_location(l):    ''' Dump data for a location to json files (from both GraphData and MapData)'''    FilesMade = 0    if l in GraphData:        with open(ext_data_path+GraphData[l]['filename'], 'w', encoding ='utf8') as outfile:            json.dump(GraphData[l], outfile)        FilesMade +=1    if l in MapData:        with open(ext_data_path+MapData[l]['filename'], 'w', encoding ='utf8') as outfile:            json.dump(MapData[l], outfile)           FilesMade +=1            return FilesMade# %%'''------------------------------------------------------------------------------Make filenames and json datafilesUse short hash for filenames to generate a short filenames that uniquely (one-to-one and onto) correspond to the location strings.This allows consistent file naming if locations are added or removed.Of course, the file name will change if the location name is changed. ------------------------------------------------------------------------------'''ext_data_path = "./tmp_data/site/plots/data/" location_to_filename = {}filename_to_location = {}print("Generate Filenames:")t0 = pnow()for l in GraphData:    h = short_hash(l)    location_to_filename[l] = h    filename_to_location[h] = l        GraphData[l]['filename'] = h+'.json'    if l in MapData:        MapData[l]['filename'] = h+'_map.json'print("    Completed in :{} sec".format(pnow()-t0))N = len(GraphData)n = 0N = len(GraphData)Ncpu = min([mp.cpu_count(),N]) # use maximal number of local CPUschunksize = 1pool = mp.Pool(processes=Ncpu)print("Dump Data To json Files For All Locations:")t0 = pnow()for n,d in enumerate(pool.imap_unordered(mp_dump_location,GraphData,chunksize=chunksize)):    if n%15==0:        pbar.progress_bar(n,N-1)    pass    if d==0:        print("No output file made for (location not found): "+l)pbar.progress_bar(n,N-1)pool.terminate()# save the translation files            with open(ext_data_path+'location_to_filename'+'.json', 'w', encoding ='utf8') as outfile:            json.dump(location_to_filename, outfile,indent=4, sort_keys=True)with open(ext_data_path+'filename_to_location'+'.json', 'w', encoding ='utf8') as outfile:            json.dump(filename_to_location, outfile,indent=4, sort_keys=True)                if len(location_to_filename)==len(filename_to_location):    #print('Filenames are unique')    passelse:    print('Filenames are NOT unique')    print("    Completed in :{} sec".format(pnow()-t0))    print("Script Completed in :{} sec".format(now()-T0))    # # %%# """# Make the external datafiles# """# nan_code = -123456789# ext_data_path = "../site/plots/data/"# filename_key2datafilename = "key_to_filename.json"# key2datafilename = {'0Info': """Keys map to the respective datafile with the COVID data# The datafiles have 3 keys: {}# 'key': [string, the key the file is associated with for covid data, or key+'_map' for map data,# 'filename': [string, the filename],# 'data': [dictionary, the data from the respective ColumnDataSource],# 'nan_code': int/float, used to identify a nan number,# }"""}# N = len(DS_Counties_COVID) - 1 + len(DS_States_COVID) + len(DS_Counties_map)# n = 0# def dic_rep_nan(dic):#     for d in dic:#         dic[d] = np.nan_to_num(dic[d], nan=nan_code)#     return dic# def dic_recursive_rep_nan(dic):#     for d in dic:#         dic[d] = recursive_nan_to_num(dic[d], nan=nan_code)#     return dic# def recursive_nan_to_num(y,nan=0):#     if isinstance(y,(list,np.ndarray)):#         for n,e in enumerate(y):#             y[n] = recursive_nan_to_num(e,nan=nan)#     else:#         y = np.nan_to_num(y,nan=nan)#     return y# for d in DS_States_COVID:#     pbar.progress_bar(n, N)#     datafilename = 'data_' + '{:05.0f}'.format(n) + '.json'#     key2datafilename[d] = datafilename#     je.dump({'key': d,#              'filename': datafilename,#              'data': dic_rep_nan(DS_States_COVID[d].data),#              'nan_code': nan_code,#              }, ext_data_path + datafilename, human_readable=False)#     n = n + 1# for d in DS_Counties_COVID:#     pbar.progress_bar(n, N)#     datafilename = 'data_' + '{:05.0f}'.format(n) + '.json'#     key2datafilename[d] = datafilename#     je.dump({'key': d,#              'filename': datafilename,#              'data': dic_rep_nan(DS_Counties_COVID[d].data),#              'nan_code': nan_code,#              }, ext_data_path + datafilename, human_readable=False)#     n = n + 1# for d in DS_Counties_map:#     pbar.progress_bar(n, N)#     datafilename = 'data_' + '{:05.0f}'.format(n) + '.json'#     key2datafilename[d+'_map'] = datafilename#     je.dump({'key': d+'_map',#              'filename': datafilename,#              'data': dic_recursive_rep_nan(DS_Counties_map[d].data),#              'nan_code': nan_code,#              }, ext_data_path + datafilename, human_readable=False)#     n = n + 1# je.dump(key2datafilename, ext_data_path + filename_key2datafilename)